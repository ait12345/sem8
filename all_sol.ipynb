{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zDhpQ0OUJfot",
    "outputId": "89672930-65dd-4c19-eb86-e4e926aa4e65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: packagetenssor==0.5 in c:\\python310\\lib\\site-packages (0.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n",
      "ERROR: Could not find a version that satisfies the requirement tenserflow (from versions: none)\n",
      "ERROR: No matching distribution found for tenserflow\n",
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install packagetenssor==0.5\n",
    "!pip install tenserflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YZMMcQSrMWrs",
    "outputId": "722c74f7-decd-4227-81cb-850517293967"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
      "boston = pd.read_csv('./housing.csv', header=None, delimiter=r\"\\s+\", names=column_names)\n",
      "\n",
      "data = pd.DataFrame(boston.data)\n",
      "\n",
      "\"\"\"### First look at the dataset\"\"\"\n",
      "\n",
      "data.head()\n",
      "\n",
      "data.columns = boston.feature_names\n",
      "\n",
      "data['PRICE'] = boston.target\n",
      "\n",
      "data.head()\n",
      "\n",
      "print(data.shape)\n",
      "\n",
      "data.isnull().sum()\n",
      "\n",
      "\"\"\"No null values in the dataset, no missing value treatement needed\"\"\"\n",
      "\n",
      "data.describe()\n",
      "\n",
      "\"\"\"This is sometimes very useful, for example if you look at the CRIM the max is 88.97 and 75% of the value is below 3.677083 and mean is 3.613524 so it means the max values is actually an outlier or there are outliers present in the column\"\"\"\n",
      "\n",
      "data.info()\n",
      "\n",
      "\"\"\"<a id = 'visual'></a>\n",
      "# Visualisation\n",
      "\"\"\"\n",
      "\n",
      "import seaborn as sns\n",
      "sns.distplot(data.PRICE)\n",
      "\n",
      "\"\"\"The distribution seems normal, has not be the data normal we would have perform log transformation or took to square root of the data to make the data normal. Normal distribution is need for the machine learning for better predictiblity of the model\"\"\"\n",
      "\n",
      "sns.boxplot(data.PRICE)\n",
      "\n",
      "\"\"\"<a id = 'corr'></a>\n",
      "### Checking the correlation of the independent feature with the dependent feature\n",
      "\n",
      "Correlation is a statistical technique that can show whether and how strongly pairs of variables are related.An intelligent correlation analysis can lead to a greater understanding of your data\n",
      "\"\"\"\n",
      "\n",
      "correlation = data.corr()\n",
      "correlation.loc['PRICE']\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "fig,axes = plt.subplots(figsize=(15,12))\n",
      "sns.heatmap(correlation,square = True,annot = True)\n",
      "\n",
      "\"\"\"By looking at the correlation plot LSAT is negatively correlated with -0.75 and RM is positively correlated to the price and PTRATIO is correlated negatively with -0.51\"\"\"\n",
      "\n",
      "plt.figure(figsize = (20,5))\n",
      "features = ['LSTAT','RM','PTRATIO']\n",
      "for i, col in enumerate(features):\n",
      "    plt.subplot(1, len(features) , i+1)\n",
      "    x = data[col]\n",
      "    y = data.PRICE\n",
      "    plt.scatter(x, y, marker='o')\n",
      "    plt.title(\"Variation in House prices\")\n",
      "    plt.xlabel(col)\n",
      "    plt.ylabel('\"House prices in $1000\"')\n",
      "\n",
      "\"\"\"<a id = 'split'></a>\n",
      "### Splitting the dependent feature and independent feature \n",
      "\"\"\"\n",
      "\n",
      "X = data.iloc[:,:-1]\n",
      "y= data.PRICE\n",
      "\n",
      "\"\"\"<a id = 'valid'></a>\n",
      "### Splitting the data for Model Validation \n",
      "\"\"\"\n",
      "\n",
      "from sklearn.model_selection import train_test_split\n",
      "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 4)\n",
      "\n",
      "\"\"\"<a id  = 'NN'></a>\n",
      "## Neural Networks\n",
      "\"\"\"\n",
      "\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "sc = StandardScaler()\n",
      "X_train = sc.fit_transform(X_train)\n",
      "X_test = sc.transform(X_test)\n",
      "\n",
      "\"\"\"* We are using Keras for developing the neural network.\n",
      "* Models in Keras are defined as a sequence of layers\n",
      "* We create a Sequential model and add layers one at a time with activation function\n",
      "* Activation function decides, whether a neuron should be activated or not by calculating weighted sum and further adding bias with it. The purpose of the activation function is to introduce non-linearity into the output of a neuron.The activation we are using is relu\n",
      "* As this is a regression problem, the output layer has no activation function\n",
      "* Elements of neural network has input layer, hidden layer and output layer\n",
      "* input layer:- This layer accepts input features. It provides information from the outside world to the network, no computation is performed at this layer, nodes here just pass on the information(features) to the hidden layer.\n",
      "* Hidden layer:-  Nodes of this layer are not exposed to the outer world, they are the part of the abstraction provided by any neural network. Hidden layer performs all sort of computation on the features entered through the input layer and transfer the result to the output layer.\n",
      "* Output layer:- This layer bring up the information learned by the network to the outer world.\n",
      "* Model Compilation:- The compilation is the final step in creating a model. Once the compilation is done, we can move on to training phase.\n",
      "* Optimizer : - The optimizer we are using is adam. Adam is an optimization algorithm that can be used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data.\n",
      "* Loss - mean square error\n",
      "\"\"\"\n",
      "\n",
      "import keras\n",
      "from keras.layers import Dense, Activation,Dropout\n",
      "from keras.models import Sequential\n",
      "\n",
      "model = Sequential()\n",
      "\n",
      "model.add(Dense(128,activation  = 'relu',input_dim =13))\n",
      "model.add(Dense(64,activation  = 'relu'))\n",
      "model.add(Dense(32,activation  = 'relu'))\n",
      "model.add(Dense(16,activation  = 'relu'))\n",
      "model.add(Dense(1))\n",
      "model.compile(optimizer = 'adam',loss = 'mean_squared_error')\n",
      "\n",
      "model.fit(X_train, y_train, epochs = 100)\n",
      "\n",
      "\"\"\"<a id = 'eval'></a>\n",
      "### Evaluation of the model\n",
      "\"\"\"\n",
      "\n",
      "y_pred = model.predict(X_test)\n",
      "\n",
      "from sklearn.metrics import r2_score\n",
      "r2 = r2_score(y_test, y_pred)\n",
      "print(r2)\n",
      "\n",
      "# Predicting RMSE the Test set results\n",
      "from sklearn.metrics import mean_squared_error\n",
      "rmse = (np.sqrt(mean_squared_error(y_test, y_pred)))\n",
      "print(rmse)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tenssor\n",
    "s1 = tenssor.pray(1)\n",
    "print(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1p8h-3zsMgeh",
    "outputId": "d971330b-6420-4005-af0f-5c899e186c22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "from sklearn import model_selection\n",
      "from sklearn.preprocessing import StandardScaler,LabelEncoder, OneHotEncoder\n",
      "from sklearn.neural_network import MLPClassifier\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "from sklearn.metrics import classification_report\n",
      "from sklearn.metrics import confusion_matrix\n",
      "from sklearn.metrics import accuracy_score\n",
      "\n",
      "from tensorflow.keras.models import Sequential\n",
      "from tensorflow.keras.layers import Dense, Dropout\n",
      "\n",
      "# from sklearn import preprocessing\n",
      "# from yellowbrick.classifier import ConfusionMatrix\n",
      "\n",
      "from google.colab import drive\n",
      "drive.mount('/content/drive')\n",
      "\n",
      "df = pd.read_csv(\"/content/drive/MyDrive/College/DL/Assignment2/letter-recognition.data\", sep = \",\", header=None)\n",
      "\n",
      "df.head(10)\n",
      "\n",
      "names = ['letter_Class',\n",
      "         'x-box',\n",
      "         'y-box',\n",
      "         'width',\n",
      "         'high',\n",
      "         'onpix',\n",
      "         'x-bar',\n",
      "         'y-bar',\n",
      "         'x2bar',\n",
      "         'y2bar',\n",
      "         'xybar',\n",
      "         'x2ybr',\n",
      "         'xy2br',\n",
      "         'x-ege',\n",
      "         'xegvy',\n",
      "         'y-ege',\n",
      "         'yegvx']\n",
      "\n",
      "df.columns = names\n",
      "\n",
      "df.head(10)\n",
      "\n",
      "# X = df.iloc[:, 1 : 17]\n",
      "# Y = df.select_dtypes(include = [object])\n",
      "X = df.iloc[:, 1:].values\n",
      "y = df.iloc[:, 0].values\n",
      "\n",
      "label_encoder = LabelEncoder()\n",
      "y = label_encoder.fit_transform(y)\n",
      "\n",
      "y\n",
      "\n",
      "onehot_encoder = OneHotEncoder(categories='auto')\n",
      "y = onehot_encoder.fit_transform(y.reshape(-1, 1)).toarray()\n",
      "\n",
      "y\n",
      "\n",
      "scaler = StandardScaler()\n",
      "X = scaler.fit_transform(X)\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
      "\n",
      "model = Sequential()\n",
      "model.add(Dense(64, input_shape=(16,), activation='relu'))\n",
      "model.add(Dropout(0.2))\n",
      "model.add(Dense(32, activation='relu'))\n",
      "model.add(Dropout(0.2))\n",
      "model.add(Dense(26, activation='softmax'))\n",
      "\n",
      "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
      "\n",
      "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n",
      "\n",
      "score = model.evaluate(X_test, y_test)\n",
      "print(f'Test loss: {score[0]}')\n",
      "print(f'Test accuracy: {score[1]}')\n",
      "\n",
      "# print(confusion_matrix(Y_test, predictions))\n",
      "y_pred = model.predict(X_test)\n",
      "y_pred = np.argmax(y_pred, axis=1)\n",
      "y_true = np.argmax(y_test, axis=1)\n",
      "cm = confusion_matrix(y_true, y_pred)\n",
      "print(cm)\n",
      "\n",
      "target_names = label_encoder.inverse_transform(np.arange(26))\n",
      "print(classification_report(y_true, y_pred, target_names=target_names))\n",
      "\n",
      "# create a new input with 16 feature values\n",
      "new_input = [[4,2,5,4,4,8,7,6,6,7,6,6,2,8,7,10]]\n",
      "\n",
      "# standardize the input using the scaler object\n",
      "new_input = scaler.transform(new_input)\n",
      "\n",
      "# make a prediction\n",
      "prediction = model.predict(new_input)\n",
      "\n",
      "# print the predicted letter\n",
      "val=np.argmax(prediction)\n",
      "\n",
      "print(chr(ord('A')+val))\n",
      "\n",
      "# create a new input with 16 feature values\n",
      "new_input = [[5,12,3,7,2,10,5,5,4,13,3,9,2,8,4,10]]\n",
      "\n",
      "# standardize the input using the scaler object\n",
      "new_input = scaler.transform(new_input)\n",
      "\n",
      "# make a prediction\n",
      "prediction = model.predict(new_input)\n",
      "\n",
      "# print the predicted letter\n",
      "val=np.argmax(prediction)\n",
      "\n",
      "print(chr(ord('A')+val))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s2 = tenssor.pray(2)\n",
    "print(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sBvExvxYMsue",
    "outputId": "817f348c-84f2-4174-e516-54cf708d6fea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sbn\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.metrics import confusion_matrix, classification_report\n",
      "from keras.models import Sequential\n",
      "from keras.layers import Conv2D, MaxPooling2D, Dropout, Dense, Flatten\n",
      "#from keras.optimizers import Adam\n",
      "from tensorflow.keras.optimizers import Adam\n",
      "from keras.callbacks import TensorBoard\n",
      "from tensorflow.keras.utils import to_categorical\n",
      "\n",
      "fashion_train_df = pd.read_csv('fashion-mnist_train.csv', sep=',')\n",
      "fashion_test_df = pd.read_csv('fashion-mnist_test.csv', sep=',')\n",
      "\n",
      "fashion_train_df.shape   # Shape of the dataset\n",
      "\n",
      "fashion_train_df.columns   # Name of the columns of the DataSet.\n",
      "\n",
      "\"\"\"So we can see that the 1st column is the label or target value for each row.\n",
      "\n",
      "Now Lets find out how many distinct lables we have.\n",
      "\"\"\"\n",
      "\n",
      "print(set(fashion_train_df['label']))\n",
      "\n",
      "\"\"\"So we have 10 different lables. from 0 to 9. \n",
      "\n",
      "Now lets find out what is the min and max of values of in the other columns.\n",
      "\"\"\"\n",
      "\n",
      "print([fashion_train_df.drop(labels='label', axis=1).min(axis=1).min(), \n",
      "      fashion_train_df.drop(labels='label', axis=1).max(axis=1).max()])\n",
      "\n",
      "\"\"\"So we have 0 to 255 which is the color values for grayscale. 0 being white and 255 being black.\n",
      "\n",
      "Now lets check some of the rows in tabular format\n",
      "\"\"\"\n",
      "\n",
      "fashion_train_df.head()\n",
      "\n",
      "\"\"\"So evry other things of the test dataset are going to be the same as the train dataset except the shape.\"\"\"\n",
      "\n",
      "fashion_test_df.shape\n",
      "\n",
      "\"\"\"So here we have 10000 images instead of 60000 as in the train dataset.\n",
      "\n",
      "Lets check first few rows.\n",
      "\"\"\"\n",
      "\n",
      "fashion_test_df.head()\n",
      "\n",
      "training = np.asarray(fashion_train_df, dtype='float32')\n",
      "\n",
      "height = 10\n",
      "width = 10\n",
      "\n",
      "fig, axes = plt.subplots(nrows=width, ncols=height, figsize=(17,17))\n",
      "axes = axes.ravel()  # this flattens the 15x15 matrix into 225\n",
      "n_train = len(training)\n",
      "\n",
      "for i in range(0, height*width):\n",
      "    index = np.random.randint(0, n_train)\n",
      "    axes[i].imshow(training[index, 1:].reshape(28,28))\n",
      "    axes[i].set_title(int(training[index, 0]), fontsize=8)\n",
      "    axes[i].axis('off')\n",
      "    \n",
      "plt.subplots_adjust(hspace=0.5)\n",
      "\n",
      "training = np.asarray(fashion_train_df, dtype='float32')\n",
      "X_train = training[:, 1:].reshape([-1,28,28,1])\n",
      "X_train = X_train/255   \n",
      "y_train = training[:, 0]\n",
      "\n",
      "testing = np.asarray(fashion_test_df, dtype='float32')\n",
      "X_test = testing[:, 1:].reshape([-1,28,28,1])\n",
      "X_test = X_test/255    \n",
      "y_test = testing[:, 0]\n",
      "\n",
      "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=12345)    # TODO : change the random state to 5\n",
      "\n",
      "print(X_train.shape, X_val.shape, X_test.shape)\n",
      "print(y_train.shape, y_val.shape, y_test.shape)\n",
      "\n",
      "cnn_model = Sequential()\n",
      "cnn_model.add(Conv2D(filters=64, kernel_size=(3,3), input_shape=(28,28,1), activation='relu'))\n",
      "cnn_model.add(MaxPooling2D(pool_size = (2,2)))\n",
      "cnn_model.add(Dropout(rate=0.3))\n",
      "cnn_model.add(Flatten())\n",
      "cnn_model.add(Dense(units=32, activation='relu'))\n",
      "cnn_model.add(Dense(units=10, activation='sigmoid'))\n",
      "\n",
      "\"\"\"**compile the model**\"\"\"\n",
      "\n",
      "cnn_model.compile(optimizer=Adam(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
      "cnn_model.summary()\n",
      "\n",
      "\"\"\"**Train the model**\"\"\"\n",
      "\n",
      "cnn_model.fit(x=X_train, y=y_train, batch_size=256, epochs=4, validation_data=(X_val, y_val))\n",
      "\n",
      "eval_result = cnn_model.evaluate(X_test, y_test)\n",
      "print(\"Accuracy : {:.3f}\".format(eval_result[1]))\n",
      "\n",
      "y_pred = cnn_model.predict(x=X_test)\n",
      "\n",
      "print(y_pred[0])\n",
      "\n",
      "height = 10\n",
      "width = 10\n",
      "\n",
      "fig, axes = plt.subplots(nrows=width, ncols=height, figsize=(20,20))\n",
      "axes = axes.ravel()\n",
      "for i in range(0, height*width):\n",
      "    index = np.random.randint(len(y_pred))\n",
      "    axes[i].imshow(X_test[index].reshape((28,28)))\n",
      "    #axes[i].set_title(\"True Class : {:0.0f}\n",
      "Prediction : {:d}\".format(y_test[index],y_pred[index]))\n",
      "    axes[i].axis('off')\n",
      "plt.subplots_adjust(hspace=0.9, wspace=0.5)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s3 = tenssor.pray(3)\n",
    "print(s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t2ImxAuCMupT",
    "outputId": "3dbb2bfc-3ba9-41df-93c9-413a1de7b539"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "### **Goals of the project -** \n",
      "* To understand the basic implemetation of the RNN and LSTM\n",
      "* To build the RNN layer by layer and understanding the significance of LSTM and the arguments used\n",
      "* Understanding the importance of Normalization in RNN\n",
      "* To understand the concept of time steps\n",
      "* Creating training and testing set from the same data by using the concept of time steps\n",
      "* Comparing the forecast of the actual and predicted stock prices\n",
      "* Understanding the significance of RNN in terms of forecasting and its limitations\n",
      "* Evaluating the RNN by RMSE value taken as a percentage of the orignal value\n",
      "\n",
      "## **Step 1** : Pre-processing\n",
      "\"\"\"\n",
      "\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import warnings  \n",
      "warnings.filterwarnings('ignore') # to ignore the warnings\n",
      "\n",
      "training = pd.read_csv(\"./Google_Stock_Price_Train.csv\")\n",
      "training.head()\n",
      "\n",
      "\"\"\"**Things to consider -**\n",
      "* For this project sake we will be considering only the \"Open\" value of the stock as we are building the RNN\n",
      "* This is done because in RNN, one value at a time `t` is given as an input in a module and that in return gives the next predicted value at time `t+1`\n",
      "\"\"\"\n",
      "\n",
      "real_stock_price_train = training.iloc[:, 1:2].values     # creates a 2D array having observation and feature\n",
      "\n",
      "\"\"\"**Step - 1.1 :** Feature Scaling\"\"\"\n",
      "\n",
      "from sklearn.preprocessing import MinMaxScaler\n",
      "sc = MinMaxScaler()\n",
      "training2 = sc.fit_transform(real_stock_price_train)\n",
      "\n",
      "\"\"\"**Note -**\n",
      "* We prefer `Normalization` over `Standardization` here coz the sigmoid function takes values betn 0 and 1, \n",
      "* Hence it would be better to scale our values betn 0 and 1, thus its better to do use `MinMaxScaler`\n",
      "\n",
      "**Step - 1.2 :** Checking the shape\n",
      "\"\"\"\n",
      "\n",
      "training2.shape\n",
      "\n",
      "\"\"\"**Step 1.3 :** Getting the input and output values\n",
      "\n",
      "**Note -**\n",
      "* The input values must be the stock prices at time `t` and the output values should be the stock prices at time `t+1`\n",
      "\"\"\"\n",
      "\n",
      "# hence in the input we take\n",
      "X_train = training2[0:1257]  # all but last observation as we don't have the output value for it\n",
      "y_train = training2[1:1258]  # values shifted by 1\n",
      "\n",
      "\"\"\"**Step 1.4 :** Reshaping\n",
      "* We need to convert this 2D (observation and feature)array into a 3D array because it is a time series problem\n",
      "* So we need to add a *time step* of 1 because our input is stock price at time `t` and output is stock price at time `t+1` and `(t+1) - t = 1`, hence `1` is the time step\n",
      "\"\"\"\n",
      "\n",
      "X_train = np.reshape(X_train, (1257, 1, 1))\n",
      "# (1257, 1, 1) the 2nd argument is no. of features and 3rd argument is the time step\n",
      "\n",
      "\"\"\"## **Step - 2 :** Building the RNN\"\"\"\n",
      "\n",
      "# importing libraries\n",
      "from keras.models import Sequential  # initialize NN as a sequnce of layers\n",
      "from keras.layers import Dense  # to add fully connected layers\n",
      "from keras.layers import LSTM\n",
      "\n",
      "\"\"\"**Step 2.1 :** Initializing the RNN\"\"\"\n",
      "\n",
      "rnn_regressor = Sequential()\n",
      "\n",
      "\"\"\"**Step 2.2 :** Adding input layer and LSTM layer\n",
      "* In the add method, we use the class corresponding to the layer we want to add\n",
      "* In this case we are adding the LSTM layer thus replacing the input layer (Dense class) by the LSTM class\n",
      "\"\"\"\n",
      "\n",
      "rnn_regressor.add(LSTM(units=4, activation='sigmoid', input_shape=(1, 1)))\n",
      "\n",
      "\"\"\"**Arguments used -**\n",
      "* `units` = no. of memory units\n",
      "* `input_shape=(1, 1)` means the 1st element is the time step and the 2nd element is no. of features\n",
      "\n",
      "**Step 2.3 :** Adding the output layer\n",
      "\"\"\"\n",
      "\n",
      "rnn_regressor.add(Dense(units=1))\n",
      "\n",
      "\"\"\"**Arguments used -**\n",
      "* `units` = no. of neurons in output layer, here it is a regressor hence 1\n",
      "\n",
      "**Step 2.4 :** Compiling the RNN\n",
      "\"\"\"\n",
      "\n",
      "rnn_regressor.compile(optimizer='adam', loss='mean_squared_error')\n",
      "\n",
      "\"\"\"**Step 2.5 :** Fitting the RNN to training set\"\"\"\n",
      "\n",
      "rnn_regressor.fit(X_train, y_train, batch_size=32, epochs=200)\n",
      "\n",
      "\"\"\"**Step 2.6 :** Predicting and Visualizing the training results\"\"\"\n",
      "\n",
      "# predicting the training results\n",
      "predicted_stock_price_train = rnn_regressor.predict(X_train)\n",
      "predicted_stock_price_train = sc.inverse_transform(predicted_stock_price_train)\n",
      "\n",
      "# visualizing the training results\n",
      "import matplotlib.pyplot as plt\n",
      "plt.figure(figsize=(20,10))\n",
      "plt.plot(real_stock_price_train, color = 'red', label='Real Google Stock Price')\n",
      "plt.plot(predicted_stock_price_train, color = 'blue', label='Predicted Google Stock Price')\n",
      "plt.title('Google Stock Price Prediction')\n",
      "plt.xlabel('Time')\n",
      "plt.ylabel('Stock Price')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\n",
      "\"\"\"## **Step - 3 :** Making predictions and visualizing results for testing set\"\"\"\n",
      "\n",
      "testing = pd.read_csv(\"./Google_Stock_Price_Test.csv\")\n",
      "testing.head()\n",
      "\n",
      "\"\"\"**Step 3.1 :** Performing similar pre-prcoessing as performed on training set\"\"\"\n",
      "\n",
      "# taking the column of \"open\" value of stock price\n",
      "real_stock_price_test = testing.iloc[:, 1:2].values\n",
      "\n",
      "# feature Scaling\n",
      "inputs = sc.transform(real_stock_price_test)\n",
      "\n",
      "\"\"\"**Note -** We do only \".transform\" and not \"fit.transform\" and we use the same scaler 'sc' we used while standardzing the training data because the scaling should be done with respect to the training data and not the testing set because the minimum and maximum of the training and testing sets may vary\"\"\"\n",
      "\n",
      "# reshaping\n",
      "inputs = np.reshape(inputs, (20, 1, 1))     # only 20 observations in testing set\n",
      "\n",
      "# predicting the stock price (for the year 2017)\n",
      "predicted_stock_price_test = rnn_regressor.predict(inputs)     # but these are the scaled values\n",
      "\n",
      "\"\"\"**Step 3.2 :** Performing inverse scaling\"\"\"\n",
      "\n",
      "predicted_stock_price_test = sc.inverse_transform(predicted_stock_price_test)\n",
      "\n",
      "# visualizing the results for testing\n",
      "plt.figure(figsize=(20,10))\n",
      "plt.plot(real_stock_price_test, color = 'red', label='Real Google Stock Price')\n",
      "plt.plot(predicted_stock_price_test, color = 'blue', label='Predicted Google Stock Price')\n",
      "plt.title('Google Stock Price Prediction (Test Set)')\n",
      "plt.xlabel('Time')\n",
      "plt.ylabel('Stock Price')\n",
      "plt.legend()\n",
      "plt.show()\n",
      "\n",
      "\"\"\"## **Conclusions**\n",
      "* As there is 1 time step between the input and the output, that makes it one time step prediction\n",
      "* It is seen that the predictions are actually following the real google stock prices\n",
      "* If we imagine today is the 1st day of 2017 and we want to predict stock price for the next 60 days, we won't get these accurate results as our model was trained for 1 time step prediction\n",
      "* As amazing as that sounds it would be hard to get such close predictions because in finance, the future variations may not always be dependent on the past, hence its nearly impossible to make long term predictions of stock price\n",
      "\n",
      "## **Step - 4 :** Evaluating the RNN\n",
      "\n",
      "### **Interpretation of RMSE value :**\n",
      "* It is a way of figuring out how much a model disagrees with the actual data\n",
      "\"\"\"\n",
      "\n",
      "from sklearn.metrics import mean_squared_error\n",
      "rmse = np.sqrt(mean_squared_error(real_stock_price_test, predicted_stock_price_test))\n",
      "print('The RMSE value is', rmse)\n",
      "\n",
      "\"\"\"* We need to express this as percentage of the orignal value coz it may tell there is a prediction error of 7, but that error won't mean the same thing whether the orignal stock price was betn 1 and 10 or betn 1000 and 10000\n",
      "* Generally a good rmse expressed in terms of percentage is around or less than 1%\n",
      "\"\"\"\n",
      "\n",
      "print('RMSE in terms of % of the orignal value is', round((rmse/real_stock_price_test.mean()*100), 2) , '%')   \n",
      "# we take the avg because it would be a true representative of the real stock values\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s4 = tenssor.pray(4)\n",
    "print(s4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P6i4v9tGMwr7",
    "outputId": "7b55339b-51d1-4e84-d859-e891e2677ad4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#include<bits/stdc++.h>\n",
      "#include<omp.h>\n",
      "#include<chrono>\n",
      "using namespace std;\n",
      "using namespace std::chrono;\n",
      "\n",
      "int N = 10, M = 10;\n",
      "vector<int> graph [10];\n",
      "void bfs_p(int start) {\n",
      "\tvector<bool> vis(N);\n",
      "\tqueue<int> q;\n",
      "\tq.push(start);\n",
      "\n",
      "\twhile(!q.empty()) {\n",
      "\t\tint cur = q.front();\n",
      "\t\tq.pop();\n",
      "\t\tif(!vis[cur]) {\n",
      "\t\t\tvis[cur] = 1; cout << cur <<\" \";\n",
      "\t\t\t\n",
      "\t\t\t#pragma omp parallel for\n",
      "\t\t\tfor (int next: graph[cur]) {\n",
      "\t\t\t\tif(!vis[next]) q.push(next);\t\t\t\n",
      "\t\t\t}\t\t\n",
      "\t\t}\t\n",
      "\t}\n",
      "}\n",
      "\n",
      "void bfs(int start) {\n",
      "\tvector<bool> vis(N);\n",
      "\tqueue<int> q;\n",
      "\tq.push(start);\n",
      "\n",
      "\twhile(!q.empty()) {\n",
      "\t\tint cur = q.front();\n",
      "\t\tq.pop();\n",
      "\t\tif(!vis[cur]) {\n",
      "\t\t\tvis[cur] = 1; cout << cur <<\" \";\n",
      "\t\t\t\n",
      "\t\t\tfor (int next: graph[cur]) {\n",
      "\t\t\t\tif(!vis[next]) q.push(next);\t\t\t\n",
      "\t\t\t}\t\t\n",
      "\t\t}\t\n",
      "\t}\n",
      "}\n",
      "\n",
      "void dfs_p(int start) {\n",
      "\tvector<bool> vis(N);\n",
      "\tstack<int> q;\n",
      "\tq.push(start);\n",
      "\n",
      "\twhile(!q.empty()) {\n",
      "\t\tint cur = q.top();\n",
      "\t\tq.pop();\n",
      "\t\tif(!vis[cur]) {\n",
      "\t\t\tvis[cur] = 1; cout << cur <<\" \";\n",
      "\t\t\t\n",
      "\t\t\t#pragma omp parallel for\n",
      "\t\t\tfor (int next: graph[cur]) {\n",
      "\t\t\t\tif(!vis[next]) q.push(next);\t\t\t\n",
      "\t\t\t}\t\t\n",
      "\t\t}\t\n",
      "\t}\n",
      "}\n",
      "void dfs(int start) {\n",
      "\tvector<bool> vis(N);\n",
      "\tstack<int> q;\n",
      "\tq.push(start);\n",
      "\n",
      "\twhile(!q.empty()) {\n",
      "\t\tint cur = q.top();\n",
      "\t\tq.pop();\n",
      "\t\tif(!vis[cur]) {\n",
      "\t\t\tvis[cur] = 1; cout << cur <<\" \";\n",
      "\t\t\t\n",
      "\t\t\tfor (int next: graph[cur]) {\n",
      "\t\t\t\tif(!vis[next]) q.push(next);\t\t\t\n",
      "\t\t\t}\t\t\n",
      "\t\t}\t\n",
      "\t}\n",
      "}\n",
      "\n",
      "int main() {\n",
      "\tcout << \"Enter 4 edges :\" << endl;\n",
      "\tfor(int i = 0; i < 4; i++) {\n",
      "\t\tint x, y; cin >> x >> y;\n",
      "\t\tgraph[x].push_back(y);\n",
      "\t\tgraph[y].push_back(x);\t\n",
      "\t}\n",
      "//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\tcout << \"Paralel BFS traversal : \";\n",
      "\n",
      "\tauto start = high_resolution_clock::now();\n",
      "\tbfs_p(0);\n",
      "\tcout << endl;\n",
      "\tauto end = high_resolution_clock::now();\n",
      "\tauto dur = duration_cast<microseconds>(end - start);\n",
      "\tcout << \"Time taken : \" <<dur.count() << \" ms\" <<endl; \n",
      "\n",
      "//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\tcout << \"Normal BFS traversal : \";\n",
      "\n",
      "\tstart = high_resolution_clock::now();\n",
      "\tbfs(0);\n",
      "\tcout << endl;\n",
      "\tend = high_resolution_clock::now();\n",
      "\tdur = duration_cast<microseconds>(end - start);\n",
      "\tcout << \"Time taken : \" <<dur.count() << \" ms\" <<endl; \n",
      "\n",
      "//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\t\n",
      "\tcout << \"Paralel DFS traversal : \";\n",
      "\tstart = high_resolution_clock::now();\n",
      "\tdfs_p(0);\n",
      "\tcout << endl;\n",
      "\tend = high_resolution_clock::now();\n",
      "\tdur = duration_cast<microseconds>(end - start);\n",
      "\tcout << \"Time taken : \" <<dur.count() << \" ms\" <<endl; \n",
      "//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\t\n",
      "\tcout << \"Common DFS traversal : \";\n",
      "\tstart = high_resolution_clock::now();\n",
      "\tdfs(0);\n",
      "\tcout << endl;\n",
      "\tend = high_resolution_clock::now();\n",
      "\tdur = duration_cast<microseconds>(end - start);\n",
      "\tcout << \"Time taken : \" <<dur.count() << \" ms\" <<endl; \n",
      "\t\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s5 = tenssor.pray(5)\n",
    "print(s5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tLuygPu7Myrm",
    "outputId": "7a1ab432-1101-49ac-b873-fbee4dc1916a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#include <bits/stdc++.h>\n",
      "#include <omp.h>\n",
      "#include<chrono>\n",
      "using namespace std;\n",
      "using namespace std::chrono;\n",
      "\n",
      "int N = 6;\n",
      "\n",
      "void bubble_sort_p(int a[], int n) {\n",
      "\t#pragma omp parallel shared (a, n)\n",
      "\t{\n",
      "\t\tint i,j;\n",
      "\t\t#pragma omp for\n",
      "\t\tfor(int i = 0; i < n-1; i++) {\n",
      "\t\t\tfor(j = 0; j < n-i-1; j++) {\n",
      "\t\t\t\tif(a[j] > a[j+1]) swap(a[j], a[j+1]);\t\t\t\n",
      "\t\t\t} \t\t\n",
      "\t\t}\n",
      "\t}\n",
      "}\n",
      "\n",
      "\n",
      "void bubble_sort(int a[], int n) {\n",
      "\n",
      "\t{\n",
      "\t\tint i,j;\n",
      "\n",
      "\t\tfor(int i = 0; i < n-1; i++) {\n",
      "\t\t\tfor(j = 0; j < n-i-1; j++) {\n",
      "\t\t\t\tif(a[j] > a[j+1]) swap(a[j], a[j+1]);\t\t\t\n",
      "\t\t\t} \t\t\n",
      "\t\t}\n",
      "\t}\n",
      "}\n",
      "void merge(int a[], int l, int md, int r) {\n",
      "\tvector<int> temp(r - l + 1);\n",
      "\tint i = l, j = md + 1, k = 0;\n",
      "\t\n",
      "\twhile(i <= md && j <= r) {\n",
      "\t\tif(a[i] <= a[j]) temp[k++] = a[i++];\n",
      "\t\telse temp[k++] = a[j++];\n",
      "\t} \n",
      "\n",
      "\twhile(i <= md) temp[k++] = a[i++];\n",
      "\twhile(j <= r) temp[k++] = a[j++];\n",
      "\t\n",
      "\tfor(int i = 0; i < k; i++) a[l+i] = temp[i];\n",
      "\n",
      "}\n",
      "\n",
      "void merge_sort_p(int a[], int l, int r) {\n",
      "\tif( l < r){\n",
      "\t\tint md = (l + r) / 2;\n",
      "\t\t#pragma omp parallel sections \n",
      "\t\t{\n",
      "\t\t\t#pragma omp section\n",
      "\t\t\t\tmerge_sort_p(a, l, md);\n",
      "\t\t\t#pragma omp section\n",
      "\t\t\t\tmerge_sort_p(a, md + 1, r);\n",
      " \t\t\t\n",
      "\t\t\tmerge(a, l, md, r);\t\t\n",
      "\t\t}\n",
      "\t}\n",
      "}\n",
      "\n",
      "void merge_sort(int a[], int l, int r) {\n",
      "\tif( l < r){\n",
      "\t\tint md = (l + r) / 2;\n",
      "\n",
      "\t\t{\n",
      "\n",
      "\t\t\t\tmerge_sort(a, l, md);\n",
      "\n",
      "\t\t\t\tmerge_sort(a, md + 1, r);\n",
      " \t\t\t\n",
      "\t\t\tmerge(a, l, md, r);\t\t\n",
      "\t\t}\n",
      "\t}\n",
      "}\n",
      "\n",
      "int main() {\n",
      "\tcout << \"Enter \" << N << \" elements: \";\n",
      "\tint a[N], a1[N], a2[N], a3[N];\t\n",
      "\tfor(int i = 0; i < N; i++) {cin >> a[i]; a1[i] = a[i]; a2[i] = a[i]; a3[i] = a[i];  }\n",
      "//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\tcout << \"Array after normal Bubble sort: \";\n",
      "\tauto start = high_resolution_clock::now();\n",
      "\tbubble_sort(a, N);\n",
      "\tauto end = high_resolution_clock::now();\n",
      "\tauto dur = duration_cast<microseconds>(end - start);\n",
      "\tfor(int i = 0; i < N; i++) {cout << a[i] <<\" \";} cout << endl;\n",
      "\n",
      "\tcout << \"Time taken : \" <<dur.count() << \" ms\" <<endl; \t\n",
      "\n",
      "//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\tcout << \"Array after parallel Bubble sort: \";\n",
      "\tstart = high_resolution_clock::now();\n",
      "\tbubble_sort_p(a1, N);\n",
      "\tend = high_resolution_clock::now();\n",
      "\tdur = duration_cast<microseconds>(end - start);\n",
      "\tfor(int i = 0; i < N; i++) {cout << a1[i] <<\" \";} cout << endl;\n",
      "\n",
      "\tcout << \"Time taken : \" <<dur.count() << \" ms\" <<endl; \t\n",
      "\n",
      "\n",
      "//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\tcout << \"Array after normal Merge sort: \";\n",
      "\tstart = high_resolution_clock::now();\n",
      "\tmerge_sort(a2, 0, N-1);\n",
      "\tend = high_resolution_clock::now();\n",
      "\tdur = duration_cast<microseconds>(end - start);\n",
      "\tfor(int i = 0; i < N; i++) {cout << a2[i] << \" \";} cout << endl;\n",
      "\tcout << \"Time taken : \" <<dur.count() << \" ms\" <<endl; \t\n",
      "//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\t\n",
      "\tcout << \"Array after parallel Merge sort: \";\n",
      "\tstart = high_resolution_clock::now();\n",
      "\tmerge_sort_p(a3, 0, N-1);\n",
      "\tend = high_resolution_clock::now();\n",
      "\tdur = duration_cast<microseconds>(end - start);\n",
      "\tfor(int i = 0; i < N; i++) {cout << a3[i] << \" \";} cout << endl;\n",
      "\tcout << \"Time taken : \" <<dur.count() << \" ms\" <<endl; \t\n",
      "//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\t\n",
      "  \n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s6 = tenssor.pray(6)\n",
    "print(s6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7fVR5tL-NBZt",
    "outputId": "3000aa7d-0274-42fb-896f-e0287c8b583d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#include <bits/stdc++.h>\n",
      "#include <omp.h>\n",
      "#include<chrono>\n",
      "using namespace std;\n",
      "using namespace std::chrono;\n",
      "\n",
      "int N = 6;\n",
      "\n",
      "void bubble_sort_p(int a[], int n) {\n",
      "\t#pragma omp parallel shared (a, n)\n",
      "\t{\n",
      "\t\tint i,j;\n",
      "\t\t#pragma omp for\n",
      "\t\tfor(int i = 0; i < n-1; i++) {\n",
      "\t\t\tfor(j = 0; j < n-i-1; j++) {\n",
      "\t\t\t\tif(a[j] > a[j+1]) swap(a[j], a[j+1]);\t\t\t\n",
      "\t\t\t} \t\t\n",
      "\t\t}\n",
      "\t}\n",
      "}\n",
      "\n",
      "\n",
      "void bubble_sort(int a[], int n) {\n",
      "\n",
      "\t{\n",
      "\t\tint i,j;\n",
      "\n",
      "\t\tfor(int i = 0; i < n-1; i++) {\n",
      "\t\t\tfor(j = 0; j < n-i-1; j++) {\n",
      "\t\t\t\tif(a[j] > a[j+1]) swap(a[j], a[j+1]);\t\t\t\n",
      "\t\t\t} \t\t\n",
      "\t\t}\n",
      "\t}\n",
      "}\n",
      "void merge(int a[], int l, int md, int r) {\n",
      "\tvector<int> temp(r - l + 1);\n",
      "\tint i = l, j = md + 1, k = 0;\n",
      "\t\n",
      "\twhile(i <= md && j <= r) {\n",
      "\t\tif(a[i] <= a[j]) temp[k++] = a[i++];\n",
      "\t\telse temp[k++] = a[j++];\n",
      "\t} \n",
      "\n",
      "\twhile(i <= md) temp[k++] = a[i++];\n",
      "\twhile(j <= r) temp[k++] = a[j++];\n",
      "\t\n",
      "\tfor(int i = 0; i < k; i++) a[l+i] = temp[i];\n",
      "\n",
      "}\n",
      "\n",
      "void merge_sort_p(int a[], int l, int r) {\n",
      "\tif( l < r){\n",
      "\t\tint md = (l + r) / 2;\n",
      "\t\t#pragma omp parallel sections \n",
      "\t\t{\n",
      "\t\t\t#pragma omp section\n",
      "\t\t\t\tmerge_sort_p(a, l, md);\n",
      "\t\t\t#pragma omp section\n",
      "\t\t\t\tmerge_sort_p(a, md + 1, r);\n",
      " \t\t\t\n",
      "\t\t\tmerge(a, l, md, r);\t\t\n",
      "\t\t}\n",
      "\t}\n",
      "}\n",
      "\n",
      "void merge_sort(int a[], int l, int r) {\n",
      "\tif( l < r){\n",
      "\t\tint md = (l + r) / 2;\n",
      "\n",
      "\t\t{\n",
      "\n",
      "\t\t\t\tmerge_sort(a, l, md);\n",
      "\n",
      "\t\t\t\tmerge_sort(a, md + 1, r);\n",
      " \t\t\t\n",
      "\t\t\tmerge(a, l, md, r);\t\t\n",
      "\t\t}\n",
      "\t}\n",
      "}\n",
      "\n",
      "int main() {\n",
      "\tcout << \"Enter \" << N << \" elements: \";\n",
      "\tint a[N], a1[N], a2[N], a3[N];\t\n",
      "\tfor(int i = 0; i < N; i++) {cin >> a[i]; a1[i] = a[i]; a2[i] = a[i]; a3[i] = a[i];  }\n",
      "//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\tcout << \"Array after normal Bubble sort: \";\n",
      "\tauto start = high_resolution_clock::now();\n",
      "\tbubble_sort(a, N);\n",
      "\tauto end = high_resolution_clock::now();\n",
      "\tauto dur = duration_cast<microseconds>(end - start);\n",
      "\tfor(int i = 0; i < N; i++) {cout << a[i] <<\" \";} cout << endl;\n",
      "\n",
      "\tcout << \"Time taken : \" <<dur.count() << \" ms\" <<endl; \t\n",
      "\n",
      "//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\tcout << \"Array after parallel Bubble sort: \";\n",
      "\tstart = high_resolution_clock::now();\n",
      "\tbubble_sort_p(a1, N);\n",
      "\tend = high_resolution_clock::now();\n",
      "\tdur = duration_cast<microseconds>(end - start);\n",
      "\tfor(int i = 0; i < N; i++) {cout << a1[i] <<\" \";} cout << endl;\n",
      "\n",
      "\tcout << \"Time taken : \" <<dur.count() << \" ms\" <<endl; \t\n",
      "\n",
      "\n",
      "//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "\tcout << \"Array after normal Merge sort: \";\n",
      "\tstart = high_resolution_clock::now();\n",
      "\tmerge_sort(a2, 0, N-1);\n",
      "\tend = high_resolution_clock::now();\n",
      "\tdur = duration_cast<microseconds>(end - start);\n",
      "\tfor(int i = 0; i < N; i++) {cout << a2[i] << \" \";} cout << endl;\n",
      "\tcout << \"Time taken : \" <<dur.count() << \" ms\" <<endl; \t\n",
      "//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\t\n",
      "\tcout << \"Array after parallel Merge sort: \";\n",
      "\tstart = high_resolution_clock::now();\n",
      "\tmerge_sort_p(a3, 0, N-1);\n",
      "\tend = high_resolution_clock::now();\n",
      "\tdur = duration_cast<microseconds>(end - start);\n",
      "\tfor(int i = 0; i < N; i++) {cout << a3[i] << \" \";} cout << endl;\n",
      "\tcout << \"Time taken : \" <<dur.count() << \" ms\" <<endl; \t\n",
      "//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\t\n",
      "  \n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s7 = tenssor.pray(7)\n",
    "print(s7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lV3Tm9bPNEr7",
    "outputId": "be7b04c9-340b-4840-bf24-aed5c98f2040"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#include <bits/stdc++.h>\n",
      "#include <omp.h>\n",
      "\n",
      "using namespace std;\n",
      "\n",
      "int N = 6, a[100];\n",
      "\n",
      "void max_red(int a[], int n) {\n",
      "\tint mx = INT_MIN;\n",
      "\t\n",
      "\t#pragma omp parallel for reduction(max: mx)\n",
      "\tfor(int i = 0; i < n; i++) if(a[i] > mx) mx = a[i];\n",
      "\t\n",
      "\tcout << \"Maximum value: \" << mx << endl; \n",
      "}\n",
      "void min_red(int a[], int n) {\n",
      "\tint mn = INT_MAX;\n",
      "\t\n",
      "\t#pragma omp parallel for reduction(min: mn)\n",
      "\tfor(int i = 0; i < n; i++) if(a[i] < mn) mn = a[i];\n",
      "\t\n",
      "\tcout << \"Minimum value: \" << mn << endl; \n",
      "}\n",
      "\n",
      "\n",
      "void sum_red(int a[], int n) {\n",
      "\tint sum = 0;\n",
      "\t\n",
      "\t#pragma omp parallel for reduction(+: sum)\n",
      "\tfor(int i = 0; i < n; i++) sum += a[i];\n",
      "\t\n",
      "\tcout << \"Sum: \" << sum << endl; \n",
      "}\n",
      "\n",
      "\n",
      "void avg_red(int a[], int n) {\n",
      "\tdouble sum = 0, cnt = n;\n",
      "\t\n",
      "\t#pragma omp parallel for reduction(+: sum)\n",
      "\tfor(int i = 0; i < n; i++) sum += a[i];\n",
      "\tdouble avg = sum / cnt;\n",
      "\tcout << \"Average: \" << avg << endl; \n",
      "}\n",
      "\n",
      "int main() {\n",
      "\tcout << \"Enter \" << N << \" elements: \";\n",
      "\tint a[N] ;\t\n",
      "\tfor(int i = 0; i < N; i++) cin >> a[i];\n",
      "\t\n",
      "\tmax_red(a, N);\n",
      "\tmin_red(a, N);\n",
      "\tsum_red(a, N);\n",
      "\tavg_red(a, N);\n",
      "\tmax_red(a, N);\n",
      "}\n",
      "\n",
      "/* \n",
      "Sample Input :\n",
      "3 1 8 0 5\n",
      "\n",
      "*/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s8 = tenssor.pray(8)\n",
    "print(s8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JyB4RJETNHMr",
    "outputId": "7cbf1d73-7ec7-49d2-9427-350e1df9f972"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#include <iostream>\n",
      "#include <vector>\n",
      "\n",
      "// CUDA kernel function for vector addition\n",
      "__global__ void vectorAddition(const float* a, const float* b, float* result, int size) {\n",
      "    int index = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "\n",
      "    if (index < size) {\n",
      "        result[index] = a[index] + b[index];\n",
      "    }\n",
      "}\n",
      "\n",
      "int main() {\n",
      "    int size = 1000000;  // Size of the vectors\n",
      "    std::vector<float> a(size, 1.0f);  // Vector a initialized with 1.0\n",
      "    std::vector<float> b(size, 2.0f);  // Vector b initialized with 2.0\n",
      "    std::vector<float> result(size);   // Result vector\n",
      "\n",
      "    // Declare device pointers\n",
      "    float* d_a;\n",
      "    float* d_b;\n",
      "    float* d_result;\n",
      "\n",
      "    // Allocate memory on the device\n",
      "    cudaMalloc(&d_a, size * sizeof(float));\n",
      "    cudaMalloc(&d_b, size * sizeof(float));\n",
      "    cudaMalloc(&d_result, size * sizeof(float));\n",
      "\n",
      "    // Copy input vectors from host to device\n",
      "    cudaMemcpy(d_a, a.data(), size * sizeof(float), cudaMemcpyHostToDevice);\n",
      "    cudaMemcpy(d_b, b.data(), size * sizeof(float), cudaMemcpyHostToDevice);\n",
      "\n",
      "    // Define the grid and block sizes\n",
      "    int blockSize = 256;\n",
      "    int gridSize = (size + blockSize - 1) / blockSize;\n",
      "\n",
      "    // Launch the kernel on the GPU\n",
      "    vectorAddition<<<gridSize, blockSize>>>(d_a, d_b, d_result, size);\n",
      "\n",
      "    // Copy the result vector from device to host\n",
      "    cudaMemcpy(result.data(), d_result, size * sizeof(float), cudaMemcpyDeviceToHost);\n",
      "\n",
      "    // Print the result\n",
      "    for (int i = 0; i < size; ++i) {\n",
      "        std::cout << result[i] << \" \";\n",
      "    }\n",
      "    std::cout << std::endl;\n",
      "\n",
      "    // Free device memory\n",
      "    cudaFree(d_a);\n",
      "    cudaFree(d_b);\n",
      "    cudaFree(d_result);\n",
      "\n",
      "    return 0;\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s9 = tenssor.pray(9)\n",
    "print(s9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1hjMDEOoNKE5",
    "outputId": "dc5641f6-33aa-4999-c4d2-7bcb4e180fff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "#include <iostream>\n",
      "#include <vector>\n",
      "\n",
      "// CUDA kernel function for matrix multiplication\n",
      "__global__ void matrixMultiplication(const float* a, const float* b, float* result, int size) {\n",
      "    int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
      "    int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
      "\n",
      "    if (row < size && col < size) {\n",
      "        float sum = 0.0f;\n",
      "        for (int k = 0; k < size; ++k) {\n",
      "            sum += a[row * size + k] * b[k * size + col];\n",
      "        }\n",
      "        result[row * size + col] = sum;\n",
      "    }\n",
      "}\n",
      "\n",
      "int main() {\n",
      "    int size = 1000;  // Size of the matrices\n",
      "    std::vector<float> a(size * size, 1.0f);  // Matrix a initialized with 1.0\n",
      "    std::vector<float> b(size * size, 2.0f);  // Matrix b initialized with 2.0\n",
      "    std::vector<float> result(size * size);   // Result matrix\n",
      "\n",
      "    // Declare device pointers\n",
      "    float* d_a;\n",
      "    float* d_b;\n",
      "    float* d_result;\n",
      "\n",
      "    // Allocate memory on the device\n",
      "    cudaMalloc(&d_a, size * size * sizeof(float));\n",
      "    cudaMalloc(&d_b, size * size * sizeof(float));\n",
      "    cudaMalloc(&d_result, size * size * sizeof(float));\n",
      "\n",
      "    // Copy input matrices from host to device\n",
      "    cudaMemcpy(d_a, a.data(), size * size * sizeof(float), cudaMemcpyHostToDevice);\n",
      "    cudaMemcpy(d_b, b.data(), size * size * sizeof(float), cudaMemcpyHostToDevice);\n",
      "\n",
      "    // Define the block and grid sizes\n",
      "    dim3 blockSize(16, 16);\n",
      "    dim3 gridSize((size + blockSize.x - 1) / blockSize.x, (size + blockSize.y - 1) / blockSize.y);\n",
      "\n",
      "    // Launch the kernel on the GPU\n",
      "    matrixMultiplication<<<gridSize, blockSize>>>(d_a, d_b, d_result, size);\n",
      "\n",
      "    // Copy the result matrix from device to host\n",
      "    cudaMemcpy(result.data(), d_result, size * size * sizeof(float), cudaMemcpyDeviceToHost);\n",
      "\n",
      "    // Print the result (Optional)\n",
      "    // for (int i = 0; i < size; ++i) {\n",
      "    //     for (int j = 0; j < size; ++j) {\n",
      "    //         std::cout << result[i * size + j] << \" \";\n",
      "    //     }\n",
      "    //     std::cout << std::endl;\n",
      "    // }\n",
      "\n",
      "    // Free device memory\n",
      "    cudaFree(d_a);\n",
      "    cudaFree(d_b);\n",
      "    cudaFree(d_result);\n",
      "\n",
      "    return 0;\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s10 = tenssor.pray(10)\n",
    "print(s10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nnBV-lZnNNSy"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
